{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8(item):\n",
    "    if isinstance(item, str):  # If item is a string\n",
    "        try:\n",
    "            return item.encode('latin1').decode('utf-8')\n",
    "        except:\n",
    "            return item  # Return original if decoding fails\n",
    "    elif isinstance(item, list):  # If item is a list\n",
    "        return [decode_utf8(x) for x in item]\n",
    "    elif isinstance(item, dict):  # If item is a dictionary\n",
    "        return {key: decode_utf8(value) for key, value in item.items()}\n",
    "    else:  # If item is neither string, list nor dict (could be int, bool, etc.)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_n_decode(data_in, file_name):\n",
    "    splited_data = data_in.replace(\"b'{\", 'b\"{').split('b\"')\n",
    "    arr_data = [spld.replace(\"\\\\'\", \"'\") for spld in splited_data]\n",
    "\n",
    "    json_data_arr = []\n",
    "    count_try, count_success = 0,0\n",
    "    \n",
    "    for i in range(len(arr_data)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        elif \"TikTokApi.video(\" in arr_data[i]:\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                count_try += 1\n",
    "                # clean & decode UTF-8\n",
    "                content = arr_data[i].replace(\"'\", '\"')[:-2]\n",
    "                content = re.sub(r'([A-Za-z])\"(?=[A-Za-z])', r\"\\1'\", content) # reg replace from Don\"t to Don't\n",
    "\n",
    "                # # analyze error\n",
    "                # pattern1 = r'\"title\":\\s*\"(.*?)\"'\n",
    "                # pattern2 = r'\"desc\":\\s*\"(.*?)\"'\n",
    "                # updated_string = re.sub(pattern1, '\"title\": \"ignore string error\"', content)\n",
    "                # updated_string = re.sub(pattern2, '\"desc\": \"ignore string error\"', updated_string).replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "\n",
    "                str_data = ast.literal_eval(content)\n",
    "                decoded_data = str(decode_utf8(str_data)).replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\").replace(\"\\\\\\\\n\", \" \")\n",
    "                updated_string = re.sub(r'([A-Za-z])\"(?=[A-Za-z])', r\"\\1'\", decoded_data) # reg replace from Don\"t to Don't\n",
    "                # print(updated_string) # print and paste this on the .json file to see if there's an error\n",
    "\n",
    "                # turn string into object\n",
    "                json_data = json.loads(updated_string)\n",
    "                id = json_data[\"id\"]\n",
    "\n",
    "                # print(id, json_data)\n",
    "\n",
    "                # # export to check json data\n",
    "                # file_path = r\"./exported-accountpage-json-error-check/\" + file_name[:-4] + \"-\" + id + \".json\"\n",
    "                # with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "                #     json.dump(json_data, file, indent=4, ensure_ascii=False)\n",
    "                \n",
    "                \n",
    "                json_data_arr.append(json_data)\n",
    "\n",
    "                count_success += 1\n",
    "                # break # uncomment this for one time loop\n",
    "            except:\n",
    "                print(file_name + ' error inside clean_n_decode() function')\n",
    "                # break\n",
    "    print(count_success, \"/\",count_try)\n",
    "    return json_data_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nested_keys(my_dict):\n",
    "\n",
    "    keys_to_extract = [\n",
    "        [\"author\", \"uniqueId\"],\n",
    "        [\"author\", \"signature\"],\n",
    "        [\"author\", \"verified\"],\n",
    "        \"desc\",\n",
    "        \"verified\",\n",
    "        \"createTime\",\n",
    "        \"digged\",\n",
    "        \"duetDisplay\",\n",
    "        \"duetEnabled\",\n",
    "        \"forFriend\",\n",
    "        \"id\",\n",
    "        \"itemCommentStatus\",\n",
    "        [\"music\", \"authorName\"],\n",
    "        [\"music\", \"coverLarge\"],\n",
    "        [\"music\", \"duration\"],\n",
    "        [\"music\", \"id\"],\n",
    "        [\"music\", \"original\"],\n",
    "        [\"music\", \"playUrl\"],\n",
    "        [\"music\", \"title\"],\n",
    "    ]\n",
    "\n",
    "    result = {}\n",
    "    for key in keys_to_extract:\n",
    "        if isinstance(key, list):\n",
    "            nested_result = my_dict\n",
    "            for nested_key in key:\n",
    "                if nested_key in nested_result:\n",
    "                    nested_result = nested_result[nested_key]\n",
    "                    # print(nested_key)\n",
    "                else:\n",
    "                    nested_result = None\n",
    "                    break\n",
    "            if nested_result is not None:\n",
    "                result[str(\"_\".join(key))] = nested_result\n",
    "        elif key in my_dict:\n",
    "            result[key] = my_dict[key]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(contents):\n",
    "    hashtag_arr = []\n",
    "\n",
    "    for content in contents:\n",
    "        for key, textExtra_arr in content.items():\n",
    "            if key == \"textExtra\":\n",
    "                for textExtra in textExtra_arr:\n",
    "                    for key, hashtagName in textExtra.items():\n",
    "                        if key == \"hashtagName\":\n",
    "                            hashtag_arr.append(hashtagName)\n",
    "\n",
    "    # print(hashtag_arr)\n",
    "    joined_string = ', '.join(hashtag_arr)\n",
    "    return joined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(json_data):\n",
    "    try:\n",
    "        extracted_values = extract_nested_keys(json_data)\n",
    "\n",
    "        if json_data[\"contents\"] is None:\n",
    "            return extracted_values\n",
    "        else:\n",
    "            extracted_values[\"hashtagNames\"] = extract_hashtags(json_data[\"contents\"])\n",
    "            return extracted_values\n",
    "    except:\n",
    "        # print(\"return without contents -> hashtagNames fields\")\n",
    "        return extracted_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(contents):\n",
    "    hashtag_arr = []\n",
    "\n",
    "    for content in contents:\n",
    "        for key, textExtra_arr in content.items():\n",
    "            if key == \"textExtra\":\n",
    "                for textExtra in textExtra_arr:\n",
    "                    for key, hashtagName in textExtra.items():\n",
    "                        if key == \"hashtagName\":\n",
    "                            hashtag_arr.append(hashtagName)\n",
    "\n",
    "    # print(hashtag_arr)\n",
    "    # joined_string = ', '.join(hashtag_arr)\n",
    "    return hashtag_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_csv_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 1997thantzinoo1.txt\n",
      "1997thantzinoo1.txt error inside clean_n_decode() function\n",
      "34 / 35\n",
      "\n",
      " 2 alocatagram.txt\n",
      "35 / 35\n",
      "\n",
      " 3 bikesure2.txt\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "bikesure2.txt error inside clean_n_decode() function\n",
      "0 / 35\n",
      "\n",
      " 4 diary.f_m.txt\n",
      "35 / 35\n",
      "\n",
      " 5 evgirl.txt\n",
      "35 / 35\n",
      "\n",
      " 6 ffern_123.txt\n",
      "35 / 35\n",
      "\n",
      " 7 flipsmile2.txt\n",
      "flipsmile2.txt error inside clean_n_decode() function\n",
      "33 / 34\n",
      "\n",
      " 8 geytty.txt\n",
      "35 / 35\n",
      "\n",
      " 9 jengtieow.txt\n",
      "jengtieow.txt error inside clean_n_decode() function\n",
      "jengtieow.txt error inside clean_n_decode() function\n",
      "jengtieow.txt error inside clean_n_decode() function\n",
      "jengtieow.txt error inside clean_n_decode() function\n",
      "jengtieow.txt error inside clean_n_decode() function\n",
      "30 / 35\n",
      "\n",
      " 10 kait000n.txt\n",
      "kait000n.txt error inside clean_n_decode() function\n",
      "34 / 35\n",
      "\n",
      " 11 kim_chiangsong.txt\n",
      "35 / 35\n",
      "\n",
      " 12 madam.grammy.txt\n",
      "35 / 35\n",
      "\n",
      " 13 nammon0012.txt\n",
      "33 / 33\n",
      "\n",
      " 14 nastya.jung.txt\n",
      "35 / 35\n",
      "\n",
      " 15 pnkkazxz1a.txt\n",
      "35 / 35\n",
      "\n",
      " 16 rattanaphon_saechio.txt\n",
      "31 / 31\n",
      "\n",
      " 17 umbrellakuns.txt\n",
      "35 / 35\n",
      "\n",
      " 18 yfamilyyy.txt\n",
      "yfamilyyy.txt error inside clean_n_decode() function\n",
      "yfamilyyy.txt error inside clean_n_decode() function\n",
      "yfamilyyy.txt error inside clean_n_decode() function\n",
      "32 / 35\n",
      "\n",
      " 19 Yok_jittraphan.txt\n",
      "35 / 35\n"
     ]
    }
   ],
   "source": [
    "origin_folder_path = r\"./txt-accounts-files\"\n",
    "csv_file_path = \"AccountPages.csv\"\n",
    "extracted_df = None\n",
    "\n",
    "clear_csv_file(csv_file_path)\n",
    "\n",
    "for i, file_name in enumerate(os.listdir(origin_folder_path)):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(origin_folder_path, file_name)\n",
    "        print(\"\\n\", i+1, file_name)\n",
    "\n",
    "        # try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_data = file.read()\n",
    "        \n",
    "        # clean and decode -> extract columns\n",
    "        json_data_arr = clean_n_decode(file_data, file_name)\n",
    "        extracted_data_json_list = [extract_data(json_data) for json_data in json_data_arr] \n",
    "        \n",
    "        # create dataframe\n",
    "        if extracted_df is None:\n",
    "            extracted_df = pd.DataFrame(columns=extracted_data_json_list[0].keys())\n",
    "\n",
    "        filtered_list = [data for data in extracted_data_json_list if data is not None] # one or more elements in your extracted_data_json_list is None instead of a dictionary\n",
    "        df = pd.DataFrame.from_records(filtered_list)\n",
    "        extracted_df = pd.concat([extracted_df, df], ignore_index=True)\n",
    "\n",
    "        # except:\n",
    "        #     print(file_name + ' error')\n",
    "\n",
    "extracted_df.to_csv(csv_file_path, mode='a', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
